{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60c2d9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75886720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>airline</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>No Reason</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>plus added commercial experience tacky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>really aggressive blast obnoxious entertainmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>really big bad thing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica seriously would pay $30 a fligh...</td>\n",
       "      <td>seriously pay flight seat did playing really b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>No Reason</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica yes, nearly every time I fly VX...</td>\n",
       "      <td>yes nearly time fly vx ear worm away</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment negativereason         airline  retweet_count  \\\n",
       "0          positive      No Reason  Virgin America              0   \n",
       "1          negative     Bad Flight  Virgin America              0   \n",
       "2          negative     Can't Tell  Virgin America              0   \n",
       "3          negative     Can't Tell  Virgin America              0   \n",
       "4          positive      No Reason  Virgin America              0   \n",
       "\n",
       "                                                text  \\\n",
       "0  @VirginAmerica plus you've added commercials t...   \n",
       "1  @VirginAmerica it's really aggressive to blast...   \n",
       "2  @VirginAmerica and it's a really big bad thing...   \n",
       "3  @VirginAmerica seriously would pay $30 a fligh...   \n",
       "4  @VirginAmerica yes, nearly every time I fly VX...   \n",
       "\n",
       "                                          text_clean  \n",
       "0             plus added commercial experience tacky  \n",
       "1  really aggressive blast obnoxious entertainmen...  \n",
       "2                               really big bad thing  \n",
       "3  seriously pay flight seat did playing really b...  \n",
       "4               yes nearly time fly vx ear worm away  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/clean_data.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecff2350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>airline</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>sentiment_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>No Reason</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>plus added commercial experience tacky</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>really aggressive blast obnoxious entertainmen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>really big bad thing</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica seriously would pay $30 a fligh...</td>\n",
       "      <td>seriously pay flight seat did playing really b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>No Reason</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica yes, nearly every time I fly VX...</td>\n",
       "      <td>yes nearly time fly vx ear worm away</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment negativereason         airline  retweet_count  \\\n",
       "0          positive      No Reason  Virgin America              0   \n",
       "1          negative     Bad Flight  Virgin America              0   \n",
       "2          negative     Can't Tell  Virgin America              0   \n",
       "3          negative     Can't Tell  Virgin America              0   \n",
       "4          positive      No Reason  Virgin America              0   \n",
       "\n",
       "                                                text  \\\n",
       "0  @VirginAmerica plus you've added commercials t...   \n",
       "1  @VirginAmerica it's really aggressive to blast...   \n",
       "2  @VirginAmerica and it's a really big bad thing...   \n",
       "3  @VirginAmerica seriously would pay $30 a fligh...   \n",
       "4  @VirginAmerica yes, nearly every time I fly VX...   \n",
       "\n",
       "                                          text_clean  sentiment_num  \n",
       "0             plus added commercial experience tacky              1  \n",
       "1  really aggressive blast obnoxious entertainmen...              0  \n",
       "2                               really big bad thing              0  \n",
       "3  seriously pay flight seat did playing really b...              0  \n",
       "4               yes nearly time fly vx ear worm away              1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create new column where positive sentiment is 0, negative sentiment is 1\n",
    "df['sentiment_num'] = df.airline_sentiment.map({'positive':1, 'negative':0})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055a638e",
   "metadata": {},
   "source": [
    "## Splitting Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdf91365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11523,)\n",
      "(11523,)\n"
     ]
    }
   ],
   "source": [
    "#Create features set (just text) and target set (just sentiment number)\n",
    "X = df.text_clean\n",
    "y = df.sentiment_num\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7b88330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8066,)\n",
      "(3457,)\n",
      "(8066,)\n",
      "(3457,)\n"
     ]
    }
   ],
   "source": [
    "#split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, stratify=y, random_state=1)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928a2e39",
   "metadata": {},
   "source": [
    "## Vectorizing And Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5764724",
   "metadata": {},
   "source": [
    "### Defining Pipelines\n",
    "\n",
    "To decide on best vectorizer and classifier combination, we will define a few pipelines and use GridSearchCV to find the optimal hyperparameters. We will use accuracy as our scoring parameter for each model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54bb73d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize vectorizers and classifiers\n",
    "count_vect = CountVectorizer()\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "lr = LogisticRegression()\n",
    "nb = MultinomialNB()\n",
    "\n",
    "#Define each pipeline for each combination of vect and clf\n",
    "pipline_cv_lr = Pipeline([\n",
    "    ('vectorizer', count_vect),\n",
    "    ('classifier', lr)\n",
    "])\n",
    "\n",
    "pipline_cv_nb = Pipeline([\n",
    "    ('vectorizer', count_vect),\n",
    "    ('classifier', nb)\n",
    "])\n",
    "\n",
    "pipline_tf_lr = Pipeline([\n",
    "    ('vectorizer', tfidf_vect),\n",
    "    ('classifier', lr)\n",
    "])\n",
    "\n",
    "pipline_tf_nb = Pipeline([\n",
    "    ('vectorizer', tfidf_vect),\n",
    "    ('classifier', nb)\n",
    "])\n",
    "\n",
    "#Define paramters for each pipeline\n",
    "params_lr = {\n",
    "    'vectorizer__min_df': [3,5,10],\n",
    "    'vectorizer__ngram_range': [(1,1), (1,2)],\n",
    "    'vectorizer__max_df': [0.75, 1.0],\n",
    "    'classifier__C': [0.1, 1, 10],\n",
    "    'classifier__max_iter': [1000]\n",
    "}\n",
    "\n",
    "params_nb = {\n",
    "    'vectorizer__min_df': [3,5,10,50],\n",
    "    'vectorizer__ngram_range': [(1,1), (1,2), (1,3)],\n",
    "    'vectorizer__max_df': [0.5, 0.75, 1.0],\n",
    "    'classifier__alpha': [0.1, 1, 10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "127374f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize each gridsearchcv object\n",
    "grid_cv_lr = GridSearchCV(pipline_cv_lr, params_lr, scoring='accuracy', cv=3)\n",
    "grid_cv_nb = GridSearchCV(pipline_cv_nb, params_nb, scoring='accuracy', cv=3)\n",
    "grid_tf_lr = GridSearchCV(pipline_tf_lr, params_lr, scoring='accuracy', cv=3)\n",
    "grid_tf_nb = GridSearchCV(pipline_tf_nb, params_nb, scoring='accuracy', cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6c0c54",
   "metadata": {},
   "source": [
    "#### CountVectorizer & Logistic Regression Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08833901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer & Logistic Regression CV Performance\n",
      "Best parameters:  {'classifier__C': 1, 'classifier__max_iter': 1000, 'vectorizer__max_df': 0.75, 'vectorizer__min_df': 3, 'vectorizer__ngram_range': (1, 2)}\n",
      "Best score:  0.9055307170748552\n"
     ]
    }
   ],
   "source": [
    "grid_cv_lr.fit(X_train, y_train)\n",
    "\n",
    "print('CountVectorizer & Logistic Regression CV Performance')\n",
    "print('Best parameters: ', grid_cv_lr.best_params_)\n",
    "print('Best score: ', grid_cv_lr.best_score_ ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bb455ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9065663870407868"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect1 = CountVectorizer(max_df = grid_cv_lr.best_params_['vectorizer__max_df'], \n",
    "                        min_df = grid_cv_lr.best_params_['vectorizer__min_df'], \n",
    "                        ngram_range = grid_cv_lr.best_params_['vectorizer__ngram_range'])\n",
    "X_train_dtm1 = vect1.fit_transform(X_train)\n",
    "X_test_dtm1 = vect1.transform(X_test)\n",
    "\n",
    "lr1 = LogisticRegression(C = grid_cv_lr.best_params_['classifier__C'], max_iter = 1000)\n",
    "lr1.fit(X_train_dtm1, y_train)\n",
    "lr1.score(X_test_dtm1, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161108cc",
   "metadata": {},
   "source": [
    "#### CountVectorizer & Multinomial Naive Bayes Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee6a2932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer & Multinomial Naive Bayes CV Performance\n",
      "Best parameters:  {'classifier__alpha': 1, 'vectorizer__max_df': 0.5, 'vectorizer__min_df': 3, 'vectorizer__ngram_range': (1, 2)}\n",
      "Best score:  0.9028026808219259\n"
     ]
    }
   ],
   "source": [
    "grid_cv_nb.fit(X_train, y_train)\n",
    "\n",
    "print('CountVectorizer & Multinomial Naive Bayes CV Performance')\n",
    "print('Best parameters: ', grid_cv_nb.best_params_)\n",
    "print('Best score: ', grid_cv_nb.best_score_ ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06fb9542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9065663870407868"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect2 = CountVectorizer(max_df = grid_cv_nb.best_params_['vectorizer__max_df'], \n",
    "                        min_df = grid_cv_nb.best_params_['vectorizer__min_df'], \n",
    "                        ngram_range = grid_cv_nb.best_params_['vectorizer__ngram_range'])\n",
    "X_train_dtm2 = vect2.fit_transform(X_train)\n",
    "X_test_dtm2 = vect2.transform(X_test)\n",
    "\n",
    "nb1 = MultinomialNB(alpha = grid_cv_nb.best_params_['classifier__alpha'])\n",
    "nb1.fit(X_train_dtm2, y_train)\n",
    "nb1.score(X_test_dtm2, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3e5857",
   "metadata": {},
   "source": [
    "#### TfidfVectorizer & Logistic Regression Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb10da22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer & Logistic Regression CV Performance\n",
      "Best parameters:  {'classifier__C': 10, 'classifier__max_iter': 1000, 'vectorizer__max_df': 0.75, 'vectorizer__min_df': 3, 'vectorizer__ngram_range': (1, 1)}\n",
      "Best score:  0.9083819772795693\n"
     ]
    }
   ],
   "source": [
    "grid_tf_lr.fit(X_train, y_train)\n",
    "\n",
    "print('TfidfVectorizer & Logistic Regression CV Performance')\n",
    "print('Best parameters: ', grid_tf_lr.best_params_)\n",
    "print('Best score: ', grid_tf_lr.best_score_ ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dba278b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.898756146948221"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect3 = CountVectorizer(max_df = grid_tf_lr.best_params_['vectorizer__max_df'], \n",
    "                        min_df = grid_tf_lr.best_params_['vectorizer__min_df'], \n",
    "                        ngram_range = grid_tf_lr.best_params_['vectorizer__ngram_range'])\n",
    "X_train_dtm3 = vect3.fit_transform(X_train)\n",
    "X_test_dtm3 = vect3.transform(X_test)\n",
    "\n",
    "lr2 = LogisticRegression(C = grid_tf_lr.best_params_['classifier__C'], max_iter = 1000)\n",
    "lr2.fit(X_train_dtm3, y_train)\n",
    "lr2.score(X_test_dtm3, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6ac73d",
   "metadata": {},
   "source": [
    "#### TfidfVectorizer & Multinomial Naive Bayes Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c681e7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer & Multinomial Naive Bayes CV Performance\n",
      "Best parameters:  {'classifier__alpha': 0.1, 'vectorizer__max_df': 0.5, 'vectorizer__min_df': 3, 'vectorizer__ngram_range': (1, 3)}\n",
      "Best score:  0.8985872410821276\n"
     ]
    }
   ],
   "source": [
    "grid_tf_nb.fit(X_train, y_train)\n",
    "\n",
    "print('TfidfVectorizer & Multinomial Naive Bayes CV Performance')\n",
    "print('Best parameters: ', grid_tf_nb.best_params_)\n",
    "print('Best score: ', grid_tf_nb.best_score_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d2474ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9010702921608331"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect4 = CountVectorizer(max_df = grid_tf_nb.best_params_['vectorizer__max_df'], \n",
    "                        min_df = grid_tf_nb.best_params_['vectorizer__min_df'], \n",
    "                        ngram_range = grid_tf_nb.best_params_['vectorizer__ngram_range'])\n",
    "X_train_dtm4 = vect4.fit_transform(X_train)\n",
    "X_test_dtm4 = vect4.transform(X_test)\n",
    "\n",
    "nb2 = MultinomialNB(alpha = grid_tf_nb.best_params_['classifier__alpha'])\n",
    "nb2.fit(X_train_dtm4, y_train)\n",
    "nb2.score(X_test_dtm4, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60448dcf",
   "metadata": {},
   "source": [
    "### Evaluating the Best Model\n",
    "\n",
    "We can see that the TfidfVectorizer and Logistic Regression Combination had the highest accuracy on the training set during GridSearch cross validation, but the CountVectorizer and Logistic Regression & Naive Bayes had the highest accuracies on the testing set (they tied). We will continue with the CountVectorizer and Naive Bayes combination as naive bayes is generally a better classifier for sentiment analysis and we will be able to explore strengths of each word with this. Let's further look at the performance of this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "557cd431",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Redefine names of vectorizers, sets, and model for ease of further evaluation\n",
    "vect = CountVectorizer(max_df = grid_cv_nb.best_params_['vectorizer__max_df'], \n",
    "                              min_df = grid_cv_nb.best_params_['vectorizer__min_df'], \n",
    "                              ngram_range = grid_cv_nb.best_params_['vectorizer__ngram_range'])\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "\n",
    "nb = MultinomialNB(alpha = grid_cv_nb.best_params_['classifier__alpha'])\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "y_pred_train = nb.predict(X_train_dtm)\n",
    "y_pred_test = nb.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01100a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aa agent</th>\n",
       "      <th>aa customer</th>\n",
       "      <th>aa doe</th>\n",
       "      <th>aa employee</th>\n",
       "      <th>aa flight</th>\n",
       "      <th>aa wa</th>\n",
       "      <th>abc</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>...</th>\n",
       "      <th>young</th>\n",
       "      <th>yous</th>\n",
       "      <th>yr</th>\n",
       "      <th>yr old</th>\n",
       "      <th>yuma</th>\n",
       "      <th>yup</th>\n",
       "      <th>yyz</th>\n",
       "      <th>zero</th>\n",
       "      <th>zone</th>\n",
       "      <th>zurich</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8061</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8062</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8063</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8064</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8065</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8066 rows × 4581 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aa  aa agent  aa customer  aa doe  aa employee  aa flight  aa wa  abc  \\\n",
       "0      0         0            0       0            0          0      0    0   \n",
       "1      0         0            0       0            0          0      0    0   \n",
       "2      0         0            0       0            0          0      0    0   \n",
       "3      0         0            0       0            0          0      0    0   \n",
       "4      0         0            0       0            0          0      0    0   \n",
       "...   ..       ...          ...     ...          ...        ...    ...  ...   \n",
       "8061   0         0            0       0            0          0      0    0   \n",
       "8062   0         0            0       0            0          0      0    0   \n",
       "8063   0         0            0       0            0          0      0    0   \n",
       "8064   0         0            0       0            0          0      0    0   \n",
       "8065   0         0            0       0            0          0      0    0   \n",
       "\n",
       "      ability  able  ...  young  yous  yr  yr old  yuma  yup  yyz  zero  zone  \\\n",
       "0           0     0  ...      0     0   0       0     0    0    0     0     0   \n",
       "1           0     0  ...      0     0   0       0     0    0    0     0     0   \n",
       "2           0     0  ...      0     0   0       0     0    0    0     0     0   \n",
       "3           0     0  ...      0     0   0       0     0    0    0     0     0   \n",
       "4           0     0  ...      0     0   0       0     0    0    0     0     0   \n",
       "...       ...   ...  ...    ...   ...  ..     ...   ...  ...  ...   ...   ...   \n",
       "8061        0     0  ...      0     0   0       0     0    0    0     0     0   \n",
       "8062        0     0  ...      0     0   0       0     0    0    0     0     0   \n",
       "8063        0     0  ...      0     0   0       0     0    0    0     0     0   \n",
       "8064        0     0  ...      0     0   0       0     0    0    0     0     0   \n",
       "8065        0     0  ...      0     0   0       0     0    0    0     0     0   \n",
       "\n",
       "      zurich  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  \n",
       "...      ...  \n",
       "8061       0  \n",
       "8062       0  \n",
       "8063       0  \n",
       "8064       0  \n",
       "8065       0  \n",
       "\n",
       "[8066 rows x 4581 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look at the train set as a pandas Dataframe\n",
    "pd.DataFrame(X_train_dtm.toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "acb6d9d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84345861",
   "metadata": {},
   "source": [
    "We can see this a sparse matrix (lots of zeros). We can also see that a lot of these words are nonsensical, probably typos, etc. but they may create noise in our model. We will attempt later on to find only the most important words and use those as our feature set. First, lets see how a prediction model works on this feature set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6586713d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data: 0.934\n",
      "Accuracy on test data:     0.907\n"
     ]
    }
   ],
   "source": [
    "print (\"Accuracy on training data: %0.3f\" % (nb.score(X_train_dtm, y_train)))\n",
    "print (\"Accuracy on test data:     %0.3f\" % (nb.score(X_test_dtm, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223f5bfb",
   "metadata": {},
   "source": [
    "We can see that this model performs quite well. It also does not seem to be overfitting the training data as the accuracy on the test data looks good as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6efaed73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Let's look at the confusion matrix for the test\n",
    "cm = metrics.confusion_matrix(y_test, y_pred_test)\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.title(\"Testing Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62879d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2163     thank reply emailed customer care department e...\n",
       "7968       best pleased service far gate guarantee correct\n",
       "7532     thank correctly interpreting tweet non sarcast...\n",
       "5396                      rt fleet fleek httptconcguzdgdaq\n",
       "4849     just wanted southwest know think great used an...\n",
       "5958     jetblue thanks including u tour did pack winte...\n",
       "1897     thank reply frustration spending upgrade just ...\n",
       "3362     quick response offering assistance lack follow...\n",
       "4311     city nationwide sale ha fabulous fare buffalo ...\n",
       "3083     thanks reaching seat wa hard cushion ok short ...\n",
       "10189    really appreciate great customer service servi...\n",
       "10892                              finally got phone worry\n",
       "9646                                          bot haha lie\n",
       "2177     loyalty team basically flipped phone thanks ma...\n",
       "8965     saving grace wa flight attendant dallas wa ama...\n",
       "4523         landing ric right left atl picked delta maybe\n",
       "5444     thank really responding request sending generi...\n",
       "2466     thanks great news refund ticket bad exchange r...\n",
       "6158       channel supposed watch scandal free flyfi sweet\n",
       "5412                        rt fleet fleek httptcoiutzvhco\n",
       "Name: text_clean, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print message text for the false positives (negative tweets incorrectly labeled as positive)\n",
    "X_test[y_test < y_pred_test].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a630a6ac",
   "metadata": {},
   "source": [
    "We can see words like \"thank\", \"best\", \"quick\", \"fabulous\", \"appreciate\", etc. were in the tweets that were incorrectly labeled as positive. We can see that this classifier is not very good at picking up sarcasm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e1ce5a",
   "metadata": {},
   "source": [
    "#### Identify Strongly Predictive Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5405e6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good words\t     P(fresh | word)\n",
      "        best airline 0.94\n",
      "            passbook 0.94\n",
      "      awesome thanks 0.93\n",
      "            guy rock 0.92\n",
      "        thanks quick 0.91\n",
      "           wonderful 0.91\n",
      "         great thank 0.91\n",
      "        great flight 0.90\n",
      "           fantastic 0.90\n",
      "        thanks great 0.89\n",
      "Bad words\t     P(fresh | word)\n",
      "                fail 0.02\n",
      "        disappointed 0.02\n",
      "                  hr 0.02\n",
      "      flight delayed 0.01\n",
      "             luggage 0.01\n",
      "                rude 0.01\n",
      "           hold hour 0.01\n",
      "              online 0.01\n",
      "    flight cancelled 0.01\n",
      "               worst 0.01\n"
     ]
    }
   ],
   "source": [
    "words = np.array(vect.get_feature_names())\n",
    "\n",
    "x = np.eye(X_test_dtm.shape[1])\n",
    "probs = nb.predict_log_proba(x)[:, 0]\n",
    "ind = np.argsort(probs)\n",
    "\n",
    "good_words = words[ind[:10]]\n",
    "bad_words = words[ind[-10:]]\n",
    "\n",
    "good_prob = probs[ind[:10]]\n",
    "bad_prob = probs[ind[-10:]]\n",
    "\n",
    "pred_strength = pd.DataFrame({'tokens':words, 'probability':1-np.exp(probs)})\n",
    "\n",
    "print(\"Good words\\t     P(fresh | word)\")\n",
    "for w, p in zip(good_words, good_prob):\n",
    "    print(\"{:>20}\".format(w), \"{:.2f}\".format(1 - np.exp(p)))\n",
    "    \n",
    "print(\"Bad words\\t     P(fresh | word)\")\n",
    "for w, p in zip(bad_words, bad_prob):\n",
    "    print(\"{:>20}\".format(w), \"{:.2f}\".format(1 - np.exp(p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e227a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4520</th>\n",
       "      <td>worst</td>\n",
       "      <td>0.006799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1351</th>\n",
       "      <td>flight cancelled</td>\n",
       "      <td>0.008837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2816</th>\n",
       "      <td>online</td>\n",
       "      <td>0.009468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932</th>\n",
       "      <td>hold hour</td>\n",
       "      <td>0.009728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3397</th>\n",
       "      <td>rude</td>\n",
       "      <td>0.011161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3933</th>\n",
       "      <td>thanks quick</td>\n",
       "      <td>0.913866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1808</th>\n",
       "      <td>guy rock</td>\n",
       "      <td>0.921078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>awesome thanks</td>\n",
       "      <td>0.932399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2884</th>\n",
       "      <td>passbook</td>\n",
       "      <td>0.940880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>best airline</td>\n",
       "      <td>0.944369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4581 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                tokens  probability\n",
       "4520             worst     0.006799\n",
       "1351  flight cancelled     0.008837\n",
       "2816            online     0.009468\n",
       "1932         hold hour     0.009728\n",
       "3397              rude     0.011161\n",
       "...                ...          ...\n",
       "3933      thanks quick     0.913866\n",
       "1808          guy rock     0.921078\n",
       "279     awesome thanks     0.932399\n",
       "2884          passbook     0.940880\n",
       "365       best airline     0.944369\n",
       "\n",
       "[4581 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_strength.sort_values('probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfc33c2",
   "metadata": {},
   "source": [
    "## Modeling by Airline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672bb179",
   "metadata": {},
   "source": [
    "We now want to build the model to classify positive and negative tweets for each airline as well as find the strongly predictive words for each. To do so, we will first attempt this by splitting the dataset by airline and modeling individually. We will then try to accomplish this by adding a new column that includes both sentiment and airline, and try to model as a multi-class classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "13463fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_united = df[df['airline']=='United']\n",
    "df_usairways = df[df['airline']=='US Airways']\n",
    "df_american = df[df['airline']=='American']\n",
    "df_southwest = df[df['airline']=='Southwest']\n",
    "df_delta = df[df['airline']=='Delta']\n",
    "df_virginam = df[df['airline']=='Virgin America']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "25bba4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiments_by_airline(df_input):\n",
    "    #Define train and test sets\n",
    "    X_df = df_input.text_clean\n",
    "    y_df = df_input.sentiment_num\n",
    "\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X_df, y_df, test_size = 0.3, stratify=y_df, random_state=1)\n",
    "\n",
    "    #Use CountVectorizer with same parameters from final model before\n",
    "    vect = CountVectorizer(max_df = grid_cv_nb.best_params_['vectorizer__max_df'], \n",
    "                           min_df = grid_cv_nb.best_params_['vectorizer__min_df'], \n",
    "                           ngram_range = grid_cv_nb.best_params_['vectorizer__ngram_range'])\n",
    "    Xtr_dtm = vect.fit_transform(Xtr)\n",
    "    Xte_dtm = vect.transform(Xte)\n",
    "\n",
    "    #Train NB model and find accuracy for train and test sets\n",
    "    nb = MultinomialNB(alpha = grid_cv_nb.best_params_['classifier__alpha'])\n",
    "    nb.fit(Xtr_dtm, ytr)\n",
    "    print (\"Accuracy on training data: %0.3f\" % (nb.score(Xtr_dtm, ytr)))\n",
    "    print (\"Accuracy on test data:     %0.3f\" % (nb.score(Xte_dtm, yte)))\n",
    "    \n",
    "    #Same process as before to find top words and ngrams for positive and negative sentiments\n",
    "    words = np.array(vect.get_feature_names())\n",
    "\n",
    "    x = np.eye(Xte_dtm.shape[1])\n",
    "    probs = nb.predict_log_proba(x)[:, 0]\n",
    "    ind = np.argsort(probs)\n",
    "\n",
    "    good_words = words[ind[:15]]\n",
    "    bad_words = words[ind[-15:]]\n",
    "\n",
    "    good_prob = probs[ind[:15]]\n",
    "    bad_prob = probs[ind[-15:]]\n",
    "\n",
    "    pred_strength = pd.DataFrame({'tokens':words, 'probability':1-np.exp(probs)})\n",
    "\n",
    "    print(\"Good words\\t     P(fresh | word)\")\n",
    "    for w, p in zip(good_words, good_prob):\n",
    "        print(\"{:>20}\".format(w), \"{:.2f}\".format(1 - np.exp(p)))\n",
    "    \n",
    "    print(\"Bad words\\t     P(fresh | word)\")\n",
    "    for w, p in zip(bad_words, bad_prob):\n",
    "        print(\"{:>20}\".format(w), \"{:.2f}\".format(1 - np.exp(p)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9ec768",
   "metadata": {},
   "source": [
    "### United"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "02a134f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data: 0.937\n",
      "Accuracy on test data:     0.904\n",
      "Good words\t     P(fresh | word)\n",
      "         thanks help 0.87\n",
      "               kudos 0.81\n",
      "               worry 0.81\n",
      "               cheer 0.81\n",
      "          thank help 0.81\n",
      "            bringing 0.78\n",
      "        great flight 0.78\n",
      "           good work 0.78\n",
      "        global class 0.78\n",
      "           follow dm 0.78\n",
      "     thanks checking 0.78\n",
      "               thank 0.76\n",
      "             amazing 0.69\n",
      "           excellent 0.69\n",
      "            deserves 0.69\n",
      "Bad words\t     P(fresh | word)\n",
      "               think 0.03\n",
      "      delayed flight 0.03\n",
      "              booked 0.03\n",
      "        disappointed 0.03\n",
      "             flightr 0.03\n",
      "        late flightr 0.03\n",
      "                long 0.03\n",
      "              minute 0.03\n",
      "                hour 0.03\n",
      "             sitting 0.03\n",
      "                said 0.03\n",
      "               stuck 0.03\n",
      "           passenger 0.02\n",
      "                 say 0.02\n",
      "               worst 0.01\n"
     ]
    }
   ],
   "source": [
    "sentiments_by_airline(df_united)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94953b4f",
   "metadata": {},
   "source": [
    "We see that the positive sentiments include a lot of ngrams including thanks, excellent, checking - they are all celebrating a good experience and showing appreciation for good service. The negative sentiments include lots of words involving time and delays, as well as showing disappointment in booking experience. \n",
    "\n",
    "### US Airways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "951afaf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data: 0.949\n",
      "Accuracy on test data:     0.918\n",
      "Good words\t     P(fresh | word)\n",
      "               kudos 0.82\n",
      "           did great 0.79\n",
      "         best follow 0.79\n",
      "           wonderful 0.79\n",
      "               shout 0.75\n",
      "        great people 0.75\n",
      "              follow 0.71\n",
      "                  fa 0.69\n",
      "         thanks help 0.65\n",
      "               super 0.65\n",
      "               thank 0.65\n",
      "           great job 0.60\n",
      "              hoping 0.60\n",
      "        flight thank 0.60\n",
      "             amazing 0.60\n",
      "Bad words\t     P(fresh | word)\n",
      "                  hr 0.02\n",
      "                 doe 0.02\n",
      "                 min 0.02\n",
      "                line 0.02\n",
      "        late flightr 0.02\n",
      "             sitting 0.02\n",
      "             flightr 0.02\n",
      "                told 0.02\n",
      "           hold hour 0.02\n",
      "                mile 0.02\n",
      "               worst 0.01\n",
      "             waiting 0.01\n",
      "                hour 0.01\n",
      "              minute 0.01\n",
      "                hold 0.01\n"
     ]
    }
   ],
   "source": [
    "sentiments_by_airline(df_usairways)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5734bd",
   "metadata": {},
   "source": [
    "The positive sentiments seem to show appreciation to the staff, flight attendents, etc. while the negative sentiments again seem to show disappointment to time related things. We see the words \"mile\" and \"hold\" which may signify the customers are having a poor experience with phone service and redeeming mileage points. \n",
    "\n",
    "### American"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e8fa5238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data: 0.954\n",
      "Accuracy on test data:     0.894\n",
      "Good words\t     P(fresh | word)\n",
      "               photo 0.86\n",
      "                safe 0.81\n",
      "        great thanks 0.81\n",
      "                 aww 0.81\n",
      "           fantastic 0.81\n",
      "           excellent 0.81\n",
      "               thank 0.81\n",
      "        photo thanks 0.78\n",
      "             amazing 0.78\n",
      "               sweet 0.78\n",
      "      awesome thanks 0.78\n",
      "         thanks help 0.78\n",
      "               kudos 0.78\n",
      "          appreciate 0.72\n",
      "               great 0.69\n",
      "Bad words\t     P(fresh | word)\n",
      "           cancelled 0.03\n",
      "              answer 0.03\n",
      "    cancelled flight 0.03\n",
      "               phone 0.03\n",
      "              online 0.03\n",
      "                want 0.03\n",
      "                hour 0.02\n",
      "               worst 0.02\n",
      "    flight cancelled 0.02\n",
      "                told 0.02\n",
      "              trying 0.02\n",
      "             waiting 0.01\n",
      "                  hr 0.01\n",
      "                need 0.01\n",
      "                hold 0.01\n"
     ]
    }
   ],
   "source": [
    "sentiments_by_airline(df_american)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a141f56c",
   "metadata": {},
   "source": [
    "For American, lots of similar words, though we do see \"safe\" which may show American is doing a good job in putting an emphasis on safety for the passengers. The negative sentiments seem to largely be directed at cancelled flights and phone service which is something this airline may want to pay attention to. \n",
    "\n",
    "### Southwest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "25efa1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data: 0.915\n",
      "Accuracy on test data:     0.858\n",
      "Good words\t     P(fresh | word)\n",
      "             awesome 0.96\n",
      "            passbook 0.93\n",
      "               thank 0.91\n",
      "        best airline 0.91\n",
      "             amazing 0.90\n",
      "                 god 0.89\n",
      "              lauren 0.87\n",
      "                rock 0.87\n",
      "              oh god 0.87\n",
      "                 lol 0.87\n",
      "            favorite 0.87\n",
      "                fast 0.87\n",
      "                 fan 0.87\n",
      "             sent dm 0.85\n",
      "      love southwest 0.85\n",
      "Bad words\t     P(fresh | word)\n",
      "              answer 0.07\n",
      "              refund 0.06\n",
      "               phone 0.06\n",
      "               stuck 0.06\n",
      "               worst 0.05\n",
      "                hour 0.05\n",
      " cancelled flightled 0.05\n",
      "           flightled 0.05\n",
      "             luggage 0.05\n",
      "    flight cancelled 0.04\n",
      "                 min 0.03\n",
      "              online 0.03\n",
      "           hold hour 0.03\n",
      "                  hr 0.02\n",
      "                hold 0.02\n"
     ]
    }
   ],
   "source": [
    "sentiments_by_airline(df_southwest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75258483",
   "metadata": {},
   "source": [
    "We see the same sort of positive words here, and even a specific name of an employee most likely (seems she needs a raise!!). The negative sentiment words seem to have similar ideas of cancellations, luggage (most likely missing), as well as refund (perhaps lots of difficulty in receiving one). \n",
    "\n",
    "### Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "22b00d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data: 0.904\n",
      "Accuracy on test data:     0.840\n",
      "Good words\t     P(fresh | word)\n",
      "               thank 0.93\n",
      "        great flight 0.93\n",
      "                rock 0.91\n",
      "               great 0.88\n",
      "              thanks 0.88\n",
      "            love guy 0.87\n",
      "          bluemanity 0.87\n",
      "            guy rock 0.87\n",
      "             awesome 0.87\n",
      "         thanks help 0.85\n",
      "      awesome thanks 0.85\n",
      "            favorite 0.85\n",
      "        thanks quick 0.85\n",
      "               video 0.85\n",
      "               reply 0.85\n",
      "Bad words\t     P(fresh | word)\n",
      "               delay 0.08\n",
      "               hotel 0.08\n",
      "                late 0.08\n",
      "      flight delayed 0.08\n",
      "  cancelled flighted 0.08\n",
      "            flighted 0.08\n",
      "                left 0.08\n",
      "                  hr 0.08\n",
      "             sitting 0.07\n",
      "                told 0.07\n",
      "                  pm 0.07\n",
      "           cancelled 0.06\n",
      "         late flight 0.05\n",
      "             delayed 0.05\n",
      "                hour 0.02\n"
     ]
    }
   ],
   "source": [
    "sentiments_by_airline(df_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc068c2d",
   "metadata": {},
   "source": [
    "Similar positive words as other airline, negative words include hotel, which may indicate trouble in booking hotel due to cancellation or poor hotel choice due to cancellation, etc. \n",
    "\n",
    "### Virgin America"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "062bf152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data: 0.893\n",
      "Accuracy on test data:     0.750\n",
      "Good words\t     P(fresh | word)\n",
      "               great 0.92\n",
      "                love 0.92\n",
      "               route 0.90\n",
      "              austin 0.88\n",
      "              thanks 0.86\n",
      "            love guy 0.86\n",
      "                best 0.85\n",
      "             america 0.83\n",
      "                rock 0.83\n",
      "                wish 0.83\n",
      "              faster 0.83\n",
      "                cool 0.81\n",
      "    customer service 0.79\n",
      "             amazing 0.79\n",
      "             awesome 0.76\n",
      "Bad words\t     P(fresh | word)\n",
      "             luggage 0.15\n",
      "               phone 0.15\n",
      "                 jfk 0.15\n",
      "             website 0.15\n",
      "                need 0.15\n",
      " cancelled flightled 0.13\n",
      "             checkin 0.13\n",
      "           flightled 0.13\n",
      "              broken 0.13\n",
      "                 bag 0.12\n",
      "              trying 0.10\n",
      "               check 0.09\n",
      "                help 0.09\n",
      "           cancelled 0.08\n",
      "                seat 0.06\n"
     ]
    }
   ],
   "source": [
    "sentiments_by_airline(df_virginam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dc773e",
   "metadata": {},
   "source": [
    "Lastly, the positive words for Virgin America are similar once again, with customer service showing up as well. Negative words indicate problems with check-in, flight seats, website, phone service, as well as luggage and bags. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
